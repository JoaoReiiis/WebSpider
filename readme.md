# Web Crawler de Mobilidade Urbana

Este √© um projeto de web crawler desenvolvido com o framework Scrapy em Python. O seu principal objetivo √© navegar de forma inteligente pela web para descobrir e extrair dados de p√°ginas e artigos relacionados a temas de mobilidade urbana, cidades inteligentes e tecnologias associadas.

## üöÄ Vis√£o Geral

O crawler inicia sua jornada a partir de uma lista pr√©-definida de URLs (`start_urls`) e, de forma aut√¥noma, navega entre os links que encontra. Ele utiliza um sistema de pontua√ß√£o para decidir quais links s√£o mais relevantes, priorizando aqueles cujos endere√ßos ou textos √¢ncora contenham palavras-chave sobre mobilidade.

Os dados coletados s√£o processados e armazenados em uma √∫nica estrutura de dados, garantindo uma coleta limpa, estruturada e pronta para o uso.

## üìä Estrutura dos Dados Coletados

Os dados s√£o salvos em um banco de dados MongoDB em uma cole√ß√£o chamada **`SpiderData`**. Esta cole√ß√£o armazena dados processados, limpos e enriquecidos, prontos para an√°lise e consumo.

-   `url` (String): A URL da p√°gina.
-   `source_domain` (String): O dom√≠nio da `seed_url` que originou a coleta.
-   `crawl_date` (DateTime): A data e hora em que a p√°gina foi processada.
-   `title` (String): O t√≠tulo do artigo ou da p√°gina.
-   `description` (String): A meta descri√ß√£o da p√°gina.
-   `lang` (String): O idioma detectado da p√°gina (ex: "pt").
-   `readable_text` (String): O texto principal do artigo, limpo de tags HTML e elementos desnecess√°rios.
-   `text_length` (Integer): O n√∫mero de caracteres do `readable_text`.
-   `published_date` (DateTime): A data de publica√ß√£o do artigo, quando dispon√≠vel.
-   `links_internal` (List): Uma lista de links encontrados na p√°gina que apontam para o mesmo dom√≠nio.
-   `links_external` (List): Uma lista de links encontrados que apontam para outros dom√≠nios.
-   `redirect_chain` (List): Uma lista de URLs que mostra o caminho de redirecionamentos at√© a URL final.
-   `depth` (Integer): O n√≠vel de profundidade da navega√ß√£o a partir da URL inicial.


## üìã Pr√©-requisitos

Antes de come√ßar, certifique-se de que voc√™ tem os seguintes softwares instalados:

-   [Python 3.8+](https://www.python.org/downloads/)
-   [MongoDB](https://www.mongodb.com/try/download/community) (o banco de dados precisa estar em execu√ß√£o)

## ‚öôÔ∏è Instala√ß√£o

Siga os passos abaixo para configurar o ambiente de desenvolvimento:

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO>
    cd WebCrawler
    ```

2.  **Crie e ative um ambiente virtual (Recomendado):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # No Windows, use: .venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias do projeto:**
    ```
    Scrapy
    pymongo
    goose3
    lxml
    ```
    Em seguida, instale-o com o pip:
    ```bash
    pip install {nome da depend√™ncias}
    ```

## üîß Configura√ß√£o

As principais configura√ß√µes do crawler podem ser ajustadas no arquivo `WebCrawler/spider/spider/settings.py`.

-   **Conex√£o com o Banco de Dados:**
    -   `MONGO_URI`: A string de conex√£o do seu MongoDB.
    -   `MONGO_DATABASE`: O nome do banco de dados a ser utilizado.

-   **Palavras-chave e URLs:**
    -   Para alterar as URLs iniciais, modifique a lista `start_urls` no arquivo `WebCrawler/spider/spider/spiders/spider.py`.
    -   Para ajustar os temas de interesse, edite as listas `KEYWORDS` (termos positivos) e `NEGATIVE_KEYWORDS` (termos a serem evitados) no arquivo de `settings.py`.

## ‚ñ∂Ô∏è Como Executar

1.  **Inicie o servi√ßo do MongoDB** na sua m√°quina.

2.  **Navegue at√© o diret√≥rio do projeto Scrapy:**
    ```bash
    cd WebCrawler/spider
    ```

3.  **Execute o crawler** com o seguinte comando no seu terminal:
    ```bash
    scrapy crawl spider
    ```

O crawler come√ßar√° a execu√ß√£o, e voc√™ ver√° os logs da atividade no terminal. Os dados coletados ser√£o salvos automaticamente no MongoDB.